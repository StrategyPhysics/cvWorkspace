{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fb849d",
   "metadata": {},
   "source": [
    "# Feature Based Image Alignment using OpenCV\n",
    "\n",
    "https://learnopencv.com/image-alignment-feature-based-using-opencv-c-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1700fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98036e59",
   "metadata": {},
   "source": [
    "Features are detected using **ORB (Oriented FAST and Rotated BRIEF)** features in the two images. Although we need only 4 features to compute the homography, typically hundreds of features are detected in the two images. We control the number of features using the parameter MAX_MATCHES.\n",
    "\n",
    "ORB is a fusion of FAST keypoint detector and BRIEF descriptor with some added features to improve the performance. **FAST is Features from Accelerated Segment Test** used to detect features from the provided image. It also uses a pyramid to produce multiscale-features. Now it doesnâ€™t compute the orientation and descriptors for the features, so this is where BRIEF comes in the role.\n",
    "\n",
    "ORB uses BRIEF descriptors but as the BRIEF performs poorly with rotation. So what ORB does is to rotate the BRIEF according to the orientation of keypoints. Using the orientation of the patch, its rotation matrix is found and rotates the BRIEF to get the rotated version. ORB is an efficient alternative to SIFT or SURF algorithms used for feature extraction, in computation cost, matching performance, and mainly the patents. SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not patented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e44cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MATCHES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6139f87",
   "metadata": {},
   "source": [
    "We find the matching features in the two images, sort them by goodness of match and keep only a small percentage (GOOD_MATCH_PERCENT) of original matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84de1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignImages(im1, im2):\n",
    "\n",
    "  # Convert images to grayscale\n",
    "  im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "  im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Detect ORB features and compute descriptors.\n",
    "  orb = cv2.ORB_create(MAX_MATCHES)\n",
    "  keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "  keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "  \n",
    "  # Match features.\n",
    "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "  matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    \n",
    "  matches = list(matches)  # KJK added this, as matches is a tuple, which does not have a 'sort' attribute\n",
    "\n",
    "  # Sort matches by score\n",
    "  matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "  # Remove not so good matches\n",
    "  numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "  matches = matches[:numGoodMatches]\n",
    "\n",
    "  # Draw top matches\n",
    "  imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "  cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "  \n",
    "  # Extract location of good matches\n",
    "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "  for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "  \n",
    "  # Find homography\n",
    "  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "  # Use homography\n",
    "  height, width, channels = im2.shape\n",
    "  im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "  \n",
    "  return im1Reg, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2035a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading reference image :  D:\\imageData\\LaserClean\\Fiducial-pre.tif\n",
      "Reading image to align :  D:\\imageData\\LaserClean\\Fiducial-post.tif\n",
      "Aligning images ...\n",
      "Saving aligned image :  card-aligned.jpg\n",
      "Estimated homography : \n",
      " [[-6.39425518e-01 -5.79866662e-02  3.67339587e+03]\n",
      " [-1.05321932e+00  5.08254370e-02  5.68297200e+03]\n",
      " [-1.85308866e-04  1.08719500e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "  # Read reference image\n",
    "  refFilename = \"form.jpg\"\n",
    "  refFilename = \"card.jpg\"\n",
    "  refFilename = \"D:\\imageData\\LaserClean\\Fiducial-pre.tif\"\n",
    "  print(\"Reading reference image : \", refFilename)\n",
    "  imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)  # cv2.IMREAD_GRAYSCALE   cv2.IMREAD_COLOR\n",
    "\n",
    "  small_to_large_image_size_ratio = 0.2\n",
    "  imReference = cv2.resize( imReference, # original image\n",
    "                       (0,0), # set fx and fy, not the final size\n",
    "                       fx=small_to_large_image_size_ratio, \n",
    "                       fy=small_to_large_image_size_ratio,\n",
    "                       interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "  # Read image to be aligned\n",
    "  imFilename = \"scanned-form.jpg\"\n",
    "  imFilename = \"card-scanned.jpg\"\n",
    "  imFilename = \"D:\\imageData\\LaserClean\\Fiducial-post.tif\"\n",
    "  print(\"Reading image to align : \", imFilename);  \n",
    "  im = cv2.imread(imFilename, cv2.IMREAD_COLOR)   # cv2.IMREAD_GRAYSCALE   cv2.IMREAD_COLOR\n",
    "\n",
    "  small_to_large_image_size_ratio = 0.2\n",
    "  im = cv2.resize( im, # original image\n",
    "                       (0,0), # set fx and fy, not the final size\n",
    "                       fx=small_to_large_image_size_ratio, \n",
    "                       fy=small_to_large_image_size_ratio,\n",
    "                       interpolation=cv2.INTER_NEAREST)\n",
    "  \n",
    "  print(\"Aligning images ...\")\n",
    "  # Registered image will be resotred in imReg. \n",
    "  # The estimated homography will be stored in h. \n",
    "  imReg, h = alignImages(im, imReference)\n",
    "  \n",
    "  # Write aligned image to disk. \n",
    "  outFilename = \"card-aligned.jpg\"\n",
    "  print(\"Saving aligned image : \", outFilename); \n",
    "  cv2.imwrite(outFilename, imReg)\n",
    "\n",
    "  # Print estimated homography\n",
    "  print(\"Estimated homography : \\n\",  h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
